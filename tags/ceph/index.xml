<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ceph on Better Tomorrow with Computer Science</title>
    <link>https://insujang.github.io/tags/ceph/</link>
    <description>Recent content in ceph on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Thu, 10 Dec 2020 15:04:00 +0900</lastBuildDate><atom:link href="https://insujang.github.io/tags/ceph/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Analyzing Ceph Network Module 2</title>
      <link>https://insujang.github.io/2020-12-10/analyzing-ceph-network-module-2/</link>
      <pubDate>Thu, 10 Dec 2020 15:04:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-12-10/analyzing-ceph-network-module-2/</guid>
      <description>Ceph Network Architecture.
  In the previous post, I illustrated overall Ceph network architecture and code execution flow of initialization, specifically for servers.
This post illustrates what would occur during connection establishment, both in servers and clients.
1. Ceph Server Accepting Connection   Ceph network function execution flow for accepting a client connection.
  In the previous post, I only showed EventCenter calls a overridden do_request() function of Processor::C_processor_accept class as a callback (BTW, this callback is for accepting a connection.</description>
    </item>
    
    <item>
      <title>Analyzing Ceph Network Module 1</title>
      <link>https://insujang.github.io/2020-12-05/analyzing-ceph-network-module-1/</link>
      <pubDate>Sat, 05 Dec 2020 15:37:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-12-05/analyzing-ceph-network-module-1/</guid>
      <description>The series of &amp;ldquo;analyzing ceph network module&amp;rdquo; posts explains how Ceph daemons communicate with each other. This post explains the network architecture overview.
A Ceph storage cluster consists of roughly four types of daemons: OSD Daemons (osd), Ceph Monitors (mon), Ceph Managers (mgr), and Ceph Metadata Servers (mds).
Ceph offical document provides a very high-level diagram that depicts the Ceph architecture:
  High level Ceph architecture. [src]
  But, how specifically those libraries (librbd, librgw, libcephfs, librados) are implemented to communicate with underlying Ceph daemons in RADOS?</description>
    </item>
    
    <item>
      <title>Accelerating Ceph RPM Packaging: Using Multithreaded Compression</title>
      <link>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</link>
      <pubDate>Sat, 07 Nov 2020 19:07:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</guid>
      <description>This post explains how we can accelerate buildig a Ceph RPM package. Knowledge in the post can be generally applied to packaging all other applications, not only Ceph.
Ceph source code is managed by Github 1, and it contains several shell scripts for packaging as well. Before illustrating how these scripts work, we have to figure out how RPM packaging works.
1. RPM Packaing 101 RPM (originally stands for Red Hat Package Manager) is a package management system developed by Red Hat 2.</description>
    </item>
    
    <item>
      <title>Deploying a Ceph Development Environment Cluster</title>
      <link>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</link>
      <pubDate>Tue, 03 Nov 2020 13:09:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</guid>
      <description>This post explains how we can deploy a Ceph development cluster from Ceph source code.
 I tested it in Windows 10 + Docker for Windows with WSL2 engine + WSL2 Ubuntu 20.04.
 1. Prerequisites Two Github repositores are necessary: Ceph 1 and Ceph-dev-docker 2.
Ceph dev docker is a kind of wrapper that automates all required steps for deloying Ceph development cluster. It users Docker container to deploy the local development of Ceph.</description>
    </item>
    
    <item>
      <title>Introduction to Ceph</title>
      <link>https://insujang.github.io/2020-08-30/introduction-to-ceph/</link>
      <pubDate>Sun, 30 Aug 2020 14:16:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-30/introduction-to-ceph/</guid>
      <description>Ceph is an open-source distributed software platform 1 2. It mainly focuses on scale-out file system including storage distribution and availability.
  Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
  A ceph storage cluster roughly consists of three components:
 Ceph Storage Nodes: equip physical storage media, and Ceph Object Storage Daemons (OSDs, or ceph-osd), Ceph Monitors (ceph-mon), and Ceph Managers (ceph-mgr) manage them.</description>
    </item>
    
  </channel>
</rss>

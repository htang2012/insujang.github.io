<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Better Tomorrow with Computer Science</title>
    <link>https://insujang.github.io/</link>
    <description>Recent content on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Sat, 07 Nov 2020 19:07:00 +0900</lastBuildDate><atom:link href="https://insujang.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Accelerating Ceph RPM Packaging: Using Multithreaded Compression</title>
      <link>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</link>
      <pubDate>Sat, 07 Nov 2020 19:07:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</guid>
      <description>This post explains how we can accelerate buildig a Ceph RPM package. Knowledge in the post can be generally applied to packaging all other applications, not only Ceph.
Ceph source code is managed by Github 1, and it contains several shell scripts for packaging as well. Before illustrating how these scripts work, we have to figure out how RPM packaging works.
1. RPM Packaing 101 RPM (originally stands for Red Hat Package Manager) is a package management system developed by Red Hat 2.</description>
    </item>
    
    <item>
      <title>Deploying a Ceph Development Environment Cluster</title>
      <link>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</link>
      <pubDate>Tue, 03 Nov 2020 13:09:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</guid>
      <description>This post explains how we can deploy a Ceph development cluster from Ceph source code.
 I tested it in Windows 10 + Docker for Windows with WSL2 engine + WSL2 Ubuntu 20.04.
 1. Prerequisites Two Github repositores are necessary: Ceph 1 and Ceph-dev-docker 2.
Ceph dev docker is a kind of wrapper that automates all required steps for deloying Ceph development cluster. It users Docker container to deploy the local development of Ceph.</description>
    </item>
    
    <item>
      <title>Introduction to Ceph</title>
      <link>https://insujang.github.io/2020-08-30/introduction-to-ceph/</link>
      <pubDate>Sun, 30 Aug 2020 14:16:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-30/introduction-to-ceph/</guid>
      <description>Ceph is an open-source distributed software platform 1 2. It mainly focuses on scale-out file system including storage distribution and availability.
  Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
  A ceph storage cluster roughly consists of three components:
 Ceph Storage Nodes: equip physical storage media, and Ceph Object Storage Daemons (OSDs, or ceph-osd), Ceph Monitors (ceph-mon), and Ceph Managers (ceph-mgr) manage them.</description>
    </item>
    
    <item>
      <title>Introduction to Flatpak</title>
      <link>https://insujang.github.io/2020-08-27/introduction-to-flatpak/</link>
      <pubDate>Thu, 27 Aug 2020 17:22:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-27/introduction-to-flatpak/</guid>
      <description>Flatpak is one of app sandboxing frameworks, along with AppImage and Snap 1. Although Snap is the most famous one, I think the internal architecture of Flatpak is more reliable.
Fedora Silverblue and EndlessOS provide software installation primarily via Flathub, a central repository of Flatpak based applications 2 3.
This post briefly summarizes how to use Flatpak in terms of implementing a sample applications.
Installing Flatpak In Ubuntu distributions, there is no Flatpak preinstalled, while it is in Fedora.</description>
    </item>
    
    <item>
      <title>Dynamic Kubelet Configuration</title>
      <link>https://insujang.github.io/2020-08-24/dynamic-kubelet-configuration/</link>
      <pubDate>Mon, 24 Aug 2020 09:21:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-24/dynamic-kubelet-configuration/</guid>
      <description>Kubelet, at launch time, loads configuration files from pre-specified files. Changed configurations are not applied into the running Kubelet process during runtime, hence manual restarting Kubelet is required after modification.
Dynamic Kubelet configuration eliminates this burden, making Kubelet monitors its configuration changes and restarts when it is updated1. It uses Kubernetes a ConfigMap object.
Kubelet Flags for Dynamic Configuration Dynamic kubelet configuration is not enabled by default. To be specific, one of required configurations is missing; the following flags for Kubelet are required for dynamic configuration:</description>
    </item>
    
    <item>
      <title>Introduction to Fedora Silverblue</title>
      <link>https://insujang.github.io/2020-07-15/fedora-silverblue/</link>
      <pubDate>Wed, 15 Jul 2020 12:35:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-07-15/fedora-silverblue/</guid>
      <description>Fedora Silverblue Fedora Silverblue 1 is an immutable desktop operating system based on Fedora Linux distribution. What immutable does mean is that most directories including rootfs (/) are mounted as read-only, and user applications run in an isolated execution environment. It is a part of Atomic Host project, and share the same underlying system with Fedora CoreOS (FCOS).
For this purpose, Fedora Silverblue adopted two technologies:
 libostree (OSTree) Flatpak  libostree 2 libostree (previously called OSTree) provides git-like model for managing bootable filesystem trees (binaries), along with for deploying them and managing the bootloader configuration.</description>
    </item>
    
    <item>
      <title>Go Modules: an Alternative to GOPATH for Package Distribution</title>
      <link>https://insujang.github.io/2020-04-04/go-modules/</link>
      <pubDate>Sat, 04 Apr 2020 19:27:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-04-04/go-modules/</guid>
      <description>This post introduces Go modules, introduced in Go version 1.11.
Go Modules? Go 1.11 introduces a new dependency mangement system, Go modules (That&amp;rsquo;s why Go uses the environment variable name GO111MODULE: indicating to use Go 1.11 module).
Google introduced Go module as an alternative to GOPATH for versioning and package distribution. At first I did not understand what does it means specifically. Here is my explanaion.
Importing Packages without Go Modules Go programmers can import third-party packages (i.</description>
    </item>
    
    <item>
      <title>Programming Kubernetes CRDs</title>
      <link>https://insujang.github.io/2020-02-13/programming-kubernetes-crd/</link>
      <pubDate>Thu, 13 Feb 2020 10:13:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-02-13/programming-kubernetes-crd/</guid>
      <description>In [previous post], I briefly introduced a custom resource definition and how to create it through CLI. In this post, I introduce how to implement Go code that programatically specifies a CRD and a custom controllers that handles CRD events.
Many tutorials are exist, but not perfect 1 2 3 4 [^tutorial4]. I by myself implement a new custom controller to fully understand how it works, and introduce almost full details here.</description>
    </item>
    
    <item>
      <title>Kubernetes Custom Resource Definition (CRD)</title>
      <link>https://insujang.github.io/2020-02-11/kubernetes-custom-resource/</link>
      <pubDate>Tue, 11 Feb 2020 17:23:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-02-11/kubernetes-custom-resource/</guid>
      <description>One of main advantages of Kubernetes API is flexibility; users can add a custom resource to the Kubernetes cluster, and Kubernetes apiserver manages defined custom resources like standard resources (e.g. ReplicaSet, etc). Main introduction in Kubernetes document is in [here].
A major confusing point comes from ambiguous distinction between Custom Resource Definition (CRD) and Aggregated APIserver (AA). Even the document explains some differences of two types of implementation, it is not clearly understandable.</description>
    </item>
    
    <item>
      <title>Introduction to Programming Infiniband RDMA</title>
      <link>https://insujang.github.io/2020-02-09/introduction-to-programming-infiniband/</link>
      <pubDate>Sun, 09 Feb 2020 19:58:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-02-09/introduction-to-programming-infiniband/</guid>
      <description>This post explains the basic of RDMA programming. There are many examples and posts regarding this, however, I personally could not find enough explanations for the examples. It was hard to understand how it works, and here I summarize what I got.
Backgrounds Channel Adapter (CA) Channel adapter refers an end node in the infiniband network. It is equivalent of Ethernet network interface card (NIC), but with more features regarding Infiniband and RDMA 1.</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Better Tomorrow with Computer Science</title>
    <link>https://insujang.github.io/posts/</link>
    <description>Recent content in Posts on Better Tomorrow with Computer Science</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright>
    <lastBuildDate>Thu, 10 Dec 2020 15:04:00 +0900</lastBuildDate><atom:link href="https://insujang.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Analyzing Ceph Network Module 2</title>
      <link>https://insujang.github.io/2020-12-10/analyzing-ceph-network-module-2/</link>
      <pubDate>Thu, 10 Dec 2020 15:04:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-12-10/analyzing-ceph-network-module-2/</guid>
      <description>Ceph Network Architecture.
  In the previous post, I illustrated overall Ceph network architecture and code execution flow of initialization, specifically for servers.
This post illustrates what would occur during connection establishment, both in servers and clients.
1. Ceph Server Accepting Connection   Ceph network function execution flow for accepting a client connection.
  In the previous post, I only showed EventCenter calls a overridden do_request() function of Processor::C_processor_accept class as a callback (BTW, this callback is for accepting a connection.</description>
    </item>
    
    <item>
      <title>Analyzing Ceph Network Module 1</title>
      <link>https://insujang.github.io/2020-12-05/analyzing-ceph-network-module-1/</link>
      <pubDate>Sat, 05 Dec 2020 15:37:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-12-05/analyzing-ceph-network-module-1/</guid>
      <description>The series of &amp;ldquo;analyzing ceph network module&amp;rdquo; posts explains how Ceph daemons communicate with each other. This post explains the network architecture overview.
A Ceph storage cluster consists of roughly four types of daemons: OSD Daemons (osd), Ceph Monitors (mon), Ceph Managers (mgr), and Ceph Metadata Servers (mds).
Ceph offical document provides a very high-level diagram that depicts the Ceph architecture:
  High level Ceph architecture. [src]
  But, how specifically those libraries (librbd, librgw, libcephfs, librados) are implemented to communicate with underlying Ceph daemons in RADOS?</description>
    </item>
    
    <item>
      <title>Building Container Image inside Container using Buildah</title>
      <link>https://insujang.github.io/2020-11-09/building-container-image-inside-container-using-buildah/</link>
      <pubDate>Mon, 09 Nov 2020 15:09:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-09/building-container-image-inside-container-using-buildah/</guid>
      <description>This post explains how we build a container image inside a container, isolating all dependent packages into the container.
The introduction below clearly shows why it is required.
 Lots of people would like to build OCI/container images within a system like Kubernetes. Imagine you have a CI/CD system that is constantly building container images, a tool like Red Hat OpenShift/Kubernetes would be useful for distributing the load of builds. Until recently, most people were leaking the Docker socket into the container and then allowing the containers to do docker build.</description>
    </item>
    
    <item>
      <title>Accelerating Ceph RPM Packaging: Using Multithreaded Compression</title>
      <link>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</link>
      <pubDate>Sat, 07 Nov 2020 19:07:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-07/accelerating-ceph-rpm-packaging-using-multithreaded-compression/</guid>
      <description>This post explains how we can accelerate buildig a Ceph RPM package. Knowledge in the post can be generally applied to packaging all other applications, not only Ceph.
Ceph source code is managed by Github 1, and it contains several shell scripts for packaging as well. Before illustrating how these scripts work, we have to figure out how RPM packaging works.
1. RPM Packaing 101 RPM (originally stands for Red Hat Package Manager) is a package management system developed by Red Hat 2.</description>
    </item>
    
    <item>
      <title>Deploying a Ceph Development Environment Cluster</title>
      <link>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</link>
      <pubDate>Tue, 03 Nov 2020 13:09:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-11-03/deploying-a-ceph-development-environment-cluster/</guid>
      <description>This post explains how we can deploy a Ceph development cluster from Ceph source code.
 I tested it in Windows 10 + Docker for Windows with WSL2 engine + WSL2 Ubuntu 20.04.
 1. Prerequisites Two Github repositores are necessary: Ceph 1 and Ceph-dev-docker 2.
Ceph dev docker is a kind of wrapper that automates all required steps for deloying Ceph development cluster. It users Docker container to deploy the local development of Ceph.</description>
    </item>
    
    <item>
      <title>Introduction to Ceph</title>
      <link>https://insujang.github.io/2020-08-30/introduction-to-ceph/</link>
      <pubDate>Sun, 30 Aug 2020 14:16:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-30/introduction-to-ceph/</guid>
      <description>Ceph is an open-source distributed software platform 1 2. It mainly focuses on scale-out file system including storage distribution and availability.
  Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
    Ceph Cluster Overview. [Source]
  A ceph storage cluster roughly consists of three components:
 Ceph Storage Nodes: equip physical storage media, and Ceph Object Storage Daemons (OSDs, or ceph-osd), Ceph Monitors (ceph-mon), and Ceph Managers (ceph-mgr) manage them.</description>
    </item>
    
    <item>
      <title>Introduction to Flatpak</title>
      <link>https://insujang.github.io/2020-08-27/introduction-to-flatpak/</link>
      <pubDate>Thu, 27 Aug 2020 17:22:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-27/introduction-to-flatpak/</guid>
      <description>Flatpak is one of app sandboxing frameworks, along with AppImage and Snap 1. Although Snap is the most famous one, I think the internal architecture of Flatpak is more reliable.
Fedora Silverblue and EndlessOS provide software installation primarily via Flathub, a central repository of Flatpak based applications 2 3.
This post briefly summarizes how to use Flatpak in terms of implementing a sample applications.
Installing Flatpak In Ubuntu distributions, there is no Flatpak preinstalled, while it is in Fedora.</description>
    </item>
    
    <item>
      <title>Dynamic Kubelet Configuration</title>
      <link>https://insujang.github.io/2020-08-24/dynamic-kubelet-configuration/</link>
      <pubDate>Mon, 24 Aug 2020 09:21:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-08-24/dynamic-kubelet-configuration/</guid>
      <description>Kubelet, at launch time, loads configuration files from pre-specified files. Changed configurations are not applied into the running Kubelet process during runtime, hence manual restarting Kubelet is required after modification.
Dynamic Kubelet configuration eliminates this burden, making Kubelet monitors its configuration changes and restarts when it is updated1. It uses Kubernetes a ConfigMap object.
Kubelet Flags for Dynamic Configuration Dynamic kubelet configuration is not enabled by default. To be specific, one of required configurations is missing; the following flags for Kubelet are required for dynamic configuration:</description>
    </item>
    
    <item>
      <title>Introduction to Fedora Silverblue</title>
      <link>https://insujang.github.io/2020-07-15/fedora-silverblue/</link>
      <pubDate>Wed, 15 Jul 2020 12:35:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-07-15/fedora-silverblue/</guid>
      <description>Fedora Silverblue Fedora Silverblue 1 is an immutable desktop operating system based on Fedora Linux distribution. What immutable does mean is that most directories including rootfs (/) are mounted as read-only, and user applications run in an isolated execution environment. It is a part of Atomic Host project, and share the same underlying system with Fedora CoreOS (FCOS).
For this purpose, Fedora Silverblue adopted two technologies:
 libostree (OSTree) Flatpak  libostree 2 libostree (previously called OSTree) provides git-like model for managing bootable filesystem trees (binaries), along with for deploying them and managing the bootloader configuration.</description>
    </item>
    
    <item>
      <title>Go Modules: an Alternative to GOPATH for Package Distribution</title>
      <link>https://insujang.github.io/2020-04-04/go-modules/</link>
      <pubDate>Sat, 04 Apr 2020 19:27:00 +0900</pubDate>
      
      <guid>https://insujang.github.io/2020-04-04/go-modules/</guid>
      <description>This post introduces Go modules, introduced in Go version 1.11.
Go Modules? Go 1.11 introduces a new dependency mangement system, Go modules (That&amp;rsquo;s why Go uses the environment variable name GO111MODULE: indicating to use Go 1.11 module).
Google introduced Go module as an alternative to GOPATH for versioning and package distribution. At first I did not understand what does it means specifically. Here is my explanaion.
Importing Packages without Go Modules Go programmers can import third-party packages (i.</description>
    </item>
    
  </channel>
</rss>
